prompts file - just for the user to use

Re-read all context and instruction files (project-context.txt, database-context.txt, copilot-instructions.txt).
Summarize your understanding of the system, database schema, and current implementation state.
Identify inconsistencies or missing information across files.
Re-synchronize your internal state and reasoning with the current project setup.
From now on, base every step and answer on the definitions and constraints in these context files.
Confirm readiness with a compact outline of key modules, tables, and API routes before proceeding.

----------------------------------------------------------------------------------------

Update error handling, status of the API call and use one standard for all error messages and succescful calls for Fintegrate APIs. Use this wrapper structure:

"callStatus" - object for API call information.
"statusCode" - API code, like 201
"statusName" - API name, like Created
"statusDescription" - description of call, if success, then constant "Success", if error, then error detailed explanation.

Example of success case response for API POST: /customer/data:

{
    "data": {
        "customer_id": "93fe20dd-031b-41f9-a989-e67220d41dd9",
        "status": "ACTIVE",
        "created_at": "2025-10-23T14:34:51.467162",
    },
    "callStatus": {
        "statusCode": "201",
        "statusName": "CREATED",
        "statusDescription": "Success"
    }
}

Example of error case response for API POST: /customer/data:
{
    "data": {},
    "callStatus": {
        "statusCode": "501",
        "statusName": "NOT_IMPLEMENTED",
        "statusDescription": "The server does not support the functionality required to fulfill the request."
    }
}

Make changes to all three currently developed APIs.

----------------------------------------------------------------------------------------

PATCH /customer/change-status - if customer already in the status, that user wants to change, return error "Customer <uuid> already <status>" or similar (better phrased).

Change callStatus object and parameter names to underscore instead of camelCase to keep unity between "data" and "call_status" objects. 

----------------------------------------------------------------------------------------

1. Change call_status object name to more standardized "detail". Do not change any other parameter names.

2. Handle these kind of errors with standardized "status_code"M "status_name", "status_description":
PATCH /customer/change-status
{
  "customer_id": "70c7ca5c-2ed8-4d3a-ad01-da794861e3d7",
  "status": "japapa"
}
{
    "detail": [
        {
            "type": "string_pattern_mismatch",
            "loc": [
                "body",
                "status"
            ],
            "msg": "String should match pattern '^(ACTIVE|INACTIVE)$'",
            "input": "japapa",
            "ctx": {
                "pattern": "^(ACTIVE|INACTIVE)$"
            },
            "url": "https://errors.pydantic.dev/2.5/v/string_pattern_mismatch"
        }
    ]
}

----------------------------------------------------------------------------------------

Let's continue with "customer_events" table. There should be an entry registered upon these API calls with these event types and source services:
POST: /customer/data | "customer_creation" | "POST: /customer/data"
DELETE: /customer/data | "customer_deletion" | "DELETE: /customer/data"
PATCH: /customer/change-status | "customer_status_change" | "PATCH: /customer/change-status"

Other collumns should be filled accordingly.

----------------------------------------------------------------------------------------

Implement new API endpoints:
POST: /customer/tag
GET: /customer/tag-value
DELETE: /customer/tag
PATCH: /customer/tag-key
PATCH: /customer/tag-value

1. POST: /customer/tag
IN customer_id, tag_keys (array), tag_values (array) --key and value position in array correspond to each other
OUT detail (success), data object empty
Request example:

{
  "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
  "tag_keys": ["registration_code", "country"], --insert into "customer_tags.tag_key"
  "tag_values": ["218753689", "UK"] --insert into "customer_tags.tag_value"
}

2. GET: /customer/tag-value
IN customer_id, tag_key
OUT tag_value
Request example:
{
  "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
  "tag_key": "registration_code"
}
Response example:
{
    "data": {
        "tag_value": "218753689"
    },
    "detail": {
        "status_code": "200",
        "status_name": "OK",
        "status_description": "Success"
    }
}

3. DELETE: /customer/tag
IN customer_id, tag_key
OUT detail (success), data object empty
Request example:
{
  "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
  "tag_key": "registration_code"
}

4. PATCH: /customer/tag-key
IN customer_id, tag_key, new_tag_key
OUT detail (success), data object empty
Request example:
{
  "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
  "tag_key": "registration_code",
  "new_tag_key": "registration_number"
}

5. PATCH: /customer/tag-value
IN customer_id, tag_key, new_tag_value
OUT detail (success), data object empty
Request example:
{
  "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
  "tag_key": "registration_code",
  "new_tag_value": "458773691"
}

Additionally, Update endpoint GET: /customer/data response - include tags object. Return tags as JSON object with "key":"value" pairs.
New response example:
{
    "data": {
        "customer_id": "c224e58b-9ac3-4c79-b5dd-64dffb3e4955",
        "name": "John Williams Doe",
        "status": "ACTIVE",
        "created_at": "2025-10-23T21:16:06.741020",
        "updated_at": "2025-10-23T21:16:37.273807",
        "tags": {
            "registration_code": "218753689",
            "country": "UK"
        }
    },
    "detail": {
        "status_code": "200",
        "status_name": "OK",
        "status_description": "Success"
    }
}

----------------------------------------------------------------------------------------

Create new DB migration file with SQL commands for these changes:
1. Add "updated_at" collumn in the table "customer_tags"
2. Update table "analytics_customers" name to "customer_analytics"

A. Update related API calls accordingly to these DB migration changes.
B. Update GET: /customer/data response to return tags in "tags" object alphabetically by "tag_key"

----------------------------------------------------------------------------------------

Create new API endpoint POST: /customer/analytics
IN customer_id
OUT detail (success), data object empty

This API creates a snapshot of analytics info in "customer_analytics" table.
There can be multiple entries in "customer_analytics" table for the same customer.
The idea is to have timelapsed info of customer data and be able to do analysis of how data changed for particular customer.

----------------------------------------------------------------------------------------

Let's focus on this flow:
POST /customer/data:
  1. Validate input
  2. Write to customers table
  3. Write to customer_events table
  4. Publish message to RabbitMQ exchange
  5. Return HTTP 201

Let's discuss:
1. Is there going to be UI for RabbitMQ for me to physically see messages in the queue (similar to ActiveMQ console)?
2. Does the consumer (client side) has to trigger (GET) the message from the queue or is it send automatically?
3. I would like to focus on one thing at a time. My idea is to send a push notification to the client system via RabbitMQ in JSON or XML format. In case of putting a message "customer.created" in RabbitMQ, do I have to prepare a client side emulator to take and parse that message?

----------------------------------------------------------------------------------------

Current plan is this:
1. Add RabbitMQ to Docker Compose
2. Modify POST /customer/data to publish message after DB write
3. Create simple consumer script that prints messages to console
4. Verify flow: API call → RabbitMQ UI shows message → Consumer prints message → Message disappears from queue

I suggest this structure to the customer creation event push notification:
{
    "event_id": "5e789704-3704-44aa-a0b6-b2c5a2762f71",
    "event_type": "customer_creation",
    "data": {
        "name": "John Richard Doe", 
        "status": "ACTIVE", 
        "customer_id": "39e1fdb9-419b-4b55-b3f4-1e958c221aa0"
    },
    "metadata": {
        "created_at": "2025-10-23T21:01:14.497319"
    }
}
All values are taken from "customer_events" table. This is for step 3 I suppose.

Let's start with the implementation of step 1 - Add RabbitMQ to Docker Compose.

----------------------------------------------------------------------------------------

This TC got me thinking:

Publisher Failure Handling
1. Stop RabbitMQ: docker stop fintegrate-rabbitmq
2. POST /customer/data → Should succeed (non-blocking), customer in DB
3. Check uvicorn logs: "RabbitMQ publish failed"
4. Restart RabbitMQ: docker start fintegrate-rabbitmq

Currently, if RabbitMQ publish result: False, after Rabbit Docker restart this push notification is not published and consequently not delivered to the consumer.
I am thinking of a solution:
1. Update table "consumer_events" - add new column "publish_status": "not_published"/"published". Add another new column "published_at" to indicate time of succesful publishing.
2. Create auto JOB to scan "consumer_events" table every 5 minutes or so to check if there are "not_published" events.
3. If there are "not_published" events then try to publish them into RabbitMQ queue.

Do not code yet. Act as an integration Architect and present your thinking and suggestions on this issue.

----------------------------------------------------------------------------------------

Let's continue discussing with these pattern options:
Pattern 1: Transactional Outbox (Industry Standard)
Pattern 4: At-Least-Once with Idempotency (Pragmatic)

We have to choose one for this phase. Let's not talk about next phases as I will learn auto JOBs separately - for now let's just focus on the problem at hand.

Please compare Pattern 1 and Pattern 4. Explain concept of idempotency clearer and with example - I am not familiar with this concept.
Do not code, let's just discuss.

----------------------------------------------------------------------------------------

I would like to stay with Pattern 1: Transactional Outbox (Industry Standard). My thinking and workaround for background worker and DB overhead:
1. I would prefer Guaranteed delivery and no Data loss risk.
2. Background worker will not be needed - I will suggest creating additional API call POST: /events/resend
3. POST: /events/resend - IN period_in_days, OUT only details, success. This API triggers once a DB command to check for unpublished events and tries to publish them once and that's it. Param "period_in_days" indicates how old events should be retried sending (check on "customer_events.created_at") - to limit traversing all table.
4. This API call will be used rarely by Fintegrate system admins.
5. Event should be published once during POST: /customer/data call (as it works right now). If failure, then "publish_status" remains on pending.
5. I did not like the idea of consumer having access to server DB regarding Pattern 4: At-Least-Once + Idempotency Solution.

Changes needed:
1. Create API POST: /events/resend
2. Update DB table Update table "consumer_events" - add new columns:
- "publish_status": "pending"/"published";
- "published_at" - to indicate time of successful publishing.
- "retry_count" - original/initial value 0, when retried with POST: /events/resend, then +1
- "last_retried_at" - to indicate time of last retry
3. Upon calling POST: /events/resend, query related events (where created_at > current_date - period_in_days AND publish_status=pending)
4. POST: /events/resend will be used in the future for all kinds of events.

Do not code, just present your view on this solution as integration architect.

----------------------------------------------------------------------------------------

So, the changes needed at this point:
1. Create API POST: /events/resend
IN "period_in_days", "max_try_count" (Optional: Skip events already retried 3+ times), "event_types" (array) (Optional: Filter by event types)
OUT data.summary (object), inside summary object: "total_pending", "attempted", "succeeded", "failed", "skipped", data.failed_events (object), inside failed_events object (array of failed events): "event_id", "event_type", "try_count", "failure_reason".

2. Update DB table Update table "consumer_events" - add new columns:
- "publish_status": "pending"/"published"/"failed"; --if try_count == 10, then consider event lost and not needed anymore, i.e. publish_status=failed
- "published_at" - to indicate time of successful publishing.
- "publish_try_count" - original/initial value 1, when retried with POST: /events/resend, then +1
- "last_tried_at" - to indicate time of last retry
- "failure_reason" - NEW: Store pika exception detail

3. Upon calling POST: /events/resend, query related events (where created_at > current_date - period_in_days AND publish_status=pending AND publish_try_count < max_try_count (if provided) AND event_type == event_types (if provided))

4. Create API GET: /events/health
IN no Parameters
OUT "pending_count", "oldest_pending_age_hours", "failed_count"

5. Make change to POST: /customer/data - if initial publish try is unsuccsessful, then:
- publish_status    = pending;
- published_at      = null;
- publish_try_count = 1; --to indicate that initial try to publish was done
- last_tried_at     = [current_date];
- failure_reason    = [error message from Rabbit/PIKA].

----------------------------------------------------------------------------------------

Next steps for RabbitMQ messaging learning:
1. Dead Letter Queues (DLQ)
Current Risk: Failed consumer processing loses messages permanently.
Problem: If consumer crashes mid-processing or encounters unrecoverable error (malformed JSON, schema mismatch), message ACKed or discarded.
Solution: Configure DLQ for customer_notifications queue

2. Message Ordering Guarantees
Current State: No ordering enforcement across events for same customer.
Scenario:
POST /customer/data → customer_creation
PATCH /customer/change-status → customer_status_change
Consumer receives #2 before #1 (network timing, retry delays)
Downstream system processes status change for non-existent customer
Solution: Use RabbitMQ routing keys with customer_id hash for consistent queue assignment, or single-active-consumer feature.

3. Idempotency Keys
Current Risk: Duplicate processing on retry or network timeout.
Scenario:
POST /events/resend publishes event
RabbitMQ ACKs internally but network drops before response
Event remains "pending" in DB
Retry publishes same event_id twice
Consumer processes twice → duplicate side effects
Solution: Consumer maintains processed_events table

Learn:
Idempotency key design (use event_id from DB)
TTL-based cleanup of processed_events
Trade-off: storage vs duplicate risk

4. Circuit Breaker for Publisher
Current Behavior: Every publish attempt creates fresh connection (3 retries × 5s timeout = 15s block per failure).
Problem: RabbitMQ down for 10 minutes → every API call blocks 15s → cascading timeouts.
Solution: Circuit breaker pattern:
CLOSED: Normal operation
OPEN: Fast-fail without connection attempt after N consecutive failures
HALF-OPEN: Periodic health check to auto-recover

5. Consumer Acknowledgment Strategy
Current Implementation: Manual ACK after successful processing.
Gap: No handling for:
Partial failure (processed but downstream HTTP call failed)
Transient errors (DB deadlock → should retry)
Permanent errors (invalid UUID → should DLQ immediately)
Solution: Conditional NACK with requeue logic

Learn:
Retry with exponential backoff (x-delay plugin)
Distinguishing transient vs permanent failures
Max retry count before DLQ

6. Message Schema Versioning
Current Risk: Schema changes break consumer without rollback plan.
Scenario:
Publisher adds new_field to payload
Old consumer crashes on unexpected field (strict parsing)
Or: Publisher removes name field
Consumer expects message['data']['name'] → KeyError
Solution: Envelope pattern with version:
{
    "schema_version": "1.0",
    "event_id": "...",
    "event_type": "customer_creation",
    "data": { ... }
}

7. Publisher Confirms
Current Gap: basic_publish() returns immediately, no guarantee RabbitMQ persisted message.
Risk: Message lost if RabbitMQ crashes between basic_publish() and disk fsync.
Solution: Enable publisher confirms:
Trade-off: Throughput vs durability (synchronous confirm adds ~10ms latency).

Learn:
Publisher confirms vs transactions
Async confirm callbacks for high throughput
When to accept data loss risk vs latency cost

8. Consumer Prefetch / Concurrency
Current Setup: Single-threaded consumer with default prefetch (unlimited).
Problem: Consumer fetches all messages from queue immediately, blocking other consumers from load distribution.
Solution: Set prefetch count:

Learn:
Prefetch tuning based on processing time
Multiple consumer instances for horizontal scaling
Work queue distribution fairness

9. Message TTL & Expiration
Current State: Messages live forever if consumer down.
Risk: Queue grows unbounded during outage, memory exhaustion.
Solution: Per-message or queue-level TTL:
properties=pika.BasicProperties(
    expiration='3600000'  # 1 hour in milliseconds
)

Learn:
TTL vs DLQ interaction
Business requirements for stale event handling
Queue length limits (x-max-length)

10. Monitoring & Observability
Current Gap: No metrics on:

Message throughput (messages/sec)
Consumer lag (time between publish and consumption)
Retry rates
DLQ depth trends
Solution: Integrate RabbitMQ management API or metrics exporters:

Prometheus exporter for RabbitMQ
Consumer lag tracking (compare event.created_at to processing time)
Alert on DLQ depth > threshold

Recommended Learning Path
Immediate (Next Phase):
Implement DLQ for customer_notifications
Add idempotency check in consumer using event_id
Enable publisher confirms for durability guarantee
Short-term (Before Production):
4. Schema versioning strategy
5. Consumer prefetch tuning + multiple instances
6. Circuit breaker for publisher connection
7. Monitoring dashboard (queue depth, consumer lag)

Long-term (Scaling):
8. Message ordering for related events
9. Consumer acknowledgment strategy refinement
10. Event-driven architecture patterns (saga, CQRS)

----------------------------------------------------------------------------------------

Let's continue with:
1. Dead Letter Queues (DLQ)
Current Risk: Failed consumer processing loses messages permanently.
Problem: If consumer crashes mid-processing or encounters unrecoverable error (malformed JSON, schema mismatch), message ACKed or discarded.
Solution: Configure DLQ for customer_notifications queue.

Do not code, just present info/instructions how such queue can be made (how much programming or configuration will be needed).
I suggest having a separate queue in RabbitMQ with name "customer_notifications_DLQ"

----------------------------------------------------------------------------------------

Next step:
2. Message Ordering Guarantees
Current State: No ordering enforcement across events for same customer.
Scenario:
POST /customer/data → customer_creation
PATCH /customer/change-status → customer_status_change
Consumer receives #2 before #1 (network timing, retry delays)
Downstream system processes status change for non-existent customer
Solution: Use RabbitMQ routing keys with customer_id hash for consistent queue assignment, or single-active-consumer feature.

Is see this as non-critical function of current Fintegrate system. Nevertheless, due to learning purposes it would be great to implement this feature.
Present your ideas how to do this. Do not code, act as an integration architect and suggest your ideas.
Do not present multiple options for realization, only one - this is not yet critical feature for the system.

----------------------------------------------------------------------------------------

I see some issues with suggested functionality. Let's talk about relationship between Consumer(s), Customer(s) and Queue(s).
Consumer - theoretically external banking system that integrates with Fintegrate. Currently just internal service of Fintegrate as a simulation of external banking system. In the future, multiple Consumers may be supported and integrated with Fintegrate.
Customer - a natural or legal entity, the client of Consumer's banking system. So one Consumer will have multimple customers (relation 1:n).
Queue - message queue currently one for all possible Consumers but may be distinct for every Consumer in the future. For now let's assume one Consumer.

So, to solve the issue "No ordering enforcement across events for same customer." I would suggest combining it with next Step:

3. Idempotency Keys
Current Risk: Duplicate processing on retry or network timeout.
Scenario:
POST /events/resend publishes event
RabbitMQ ACKs internally but network drops before response
Event remains "pending" in DB
Retry publishes same event_id twice
Consumer processes twice → duplicate side effects
Solution: Consumer maintains received_events table

My sauggested colution:
1. Update table "customer_events", add new collumns:
"deliver_status" (pending, delivered, failed. Failed if message went to DLQ), "delivered_at", "deliver_try_count", "deliver_last_tried_at", "deliver_failure_reason".
2. Update table "customer_events" change current collumn names from "last_tried_at" and "failure_reason" to "publish_last_tried_at" and "publish_failure_reason" for clarity.
3. Create new table "consumer_received_events". For simplicity, I will implement consumer (client) side tables into the same Fintegrate DB. All these table names will have prefix "consumer_*"
"consumer_received_events" collumns:
"consumer_id" (for now, constant null. In the future this will be added), "event_id", "customer_id", "event_type", "receive_status" (success/failed), "received_at", "receiving_failure_reason".
4. Logic changes, upon customer creation, status_change or deletion, when new event is created, we need to check by customer_id if all events for that customer has been delivered (deliver_status).
If not, i.e. there are events where deliver_status=pending or failed, the event should not be published and "publish_failure_reason" should be filled "There are undelivered events for this customer".
5. When the consumer receives the message of the event, there should be entry created in "consumer_received_events" table.
6. When entry in "consumer_received_events" table is made upon successful delivery, "customer_events" table should be updated with delivery information for that event.
7. When POST: /events/resend API is called, it should try to publish unpublished customer events by created_at in asc order. Technically that should prevent mixing order of events in publishing.

Do not code yet, act as an integration architect, present your view on suggested solution. I understand that overhead for DB will increase but in my opinion not significantly.

----------------------------------------------------------------------------------------

Suggested changes for changes:
1. "consumer_event_receipts" table
receipt_id UUID PRIMARY KEY
consumer_id UUID                  -- NULL for now, FK later
event_id UUID REFERENCES customer_events(event_id)
customer_id UUID
event_type VARCHAR(100)
received_at TIMESTAMP
processing_status VARCHAR(20)     -- received/processed/failed
processing_failure_reason TEXT

2. API Addition: POST /events/confirm-delivery. Purpose: Consumer reports successful receipt.
Request example (consumer_id not needed at the moment - in the future all APIs will be updated with headers and authentication of consumers):
{
  "event_id": "abc-123",
  "status": "received",
  "received_at": "2025-10-28T10:30:00Z"
}
Logic:
Insert into consumer_event_receipts
Update customer_events.deliver_status='delivered', delivered_at=now()
Return 200 OK

3. Consumer Callback Modification

4. Remove Step 4 blocking logic. Instead: Consumer Processing Queue implementation.

5. API addition: POST: /events/redeliver - Manually republishes deliver_status=pending events (admin troubleshooting)

----------------------------------------------------------------------------------------

Recommended Learning Path for Fintegrate
Step 1: Add Traefik to Docker Compose (1-2 hours)
Add Traefik container
Route customer_service through gateway
Test: http://localhost/customer/data works
View Traefik dashboard at http://localhost:8080
Key Concepts: Reverse proxy, routing rules, service discovery

Step 2: Implement API Key Authentication (2-3 hours)
Create api_keys table in database
Build middleware to validate keys
Reject requests without valid key
Test with Postman: X-API-Key: your-key-here
Key Concepts: Authentication vs authorization, middleware, security headers

Step 3: Add Rate Limiting (1 hour)
Configure Traefik rate limiter
Test: Send 200 requests rapidly → see 429 errors
Implement per-IP vs per-API-key limits
Key Concepts: DDoS prevention, throttling strategies, backpressure

Step 4: Setup Monitoring (2-3 hours)
Enable Prometheus metrics
Add Grafana dashboard
Create alerts (e.g., >10% error rate)
View request latency trends
Key Concepts: Observability, SLIs/SLOs, alerting

----------------------------------------------------------------------------------------

Step 2: Implement API Key Authentication (2-3 hours)
Create api_keys table in database
Build middleware to validate keys
Reject requests without valid key
Test with Postman: X-API-Key: your-key-here
Key Concepts: Authentication vs authorization, middleware, security headers

Tasks:
1. Create new DB tables "consumers". No tags table will be needed.
2. Create new DB table "consumer_api_keys".
3. Update "customer_events" table: add collumn "consumer_id" to differentiate events of different consumers.
4. Update all current API calls to require new header to provide "X-API-Key".
5. Create new API calls: 
- POST: /consumer/data ("consumers" table - number of consumers will be low, initialy 1, later 2-3).
- POST: /consumer/api-key ("consumer_api_keys" - only one active api key per consumer).
- GET: /consumer/data
- GET: /consumer/api-key
- POST: /consumer/deactivate-api-key
- POST: /consumer/change-status
6. Differentiate Rabbit queues by consumer_name, i.e. "customer_notification_[consumer_name]", i.e. "customer_notification_consumer001".

In this prompt, let's start with task 1 and 2 and 3:
1. Create new DB table "consumers".
2. Create new DB table "consumer_api_keys".
3. Update "customer_events" table: add collumn "consumer_id" to differentiate events of different consumers.

My suggestion:
1. Table "consumers", collumns:
- consumer_id UUID
- name
- status (active, deactivated)
- created_at
- updated_at

2. Table "consumer_api_keys", collumns:
- api_key_value
- api_key_status (active, expired, deactivated)
- consumer_id
- created_at
- expires_at
- updated_at

Let's consider now only Tasks 1 and 2 and 3. Do not code, just make your suggestions and corrections (if needed) as an intergation architect (maybe I forgot or did not think about - help me to think like integration architect).

----------------------------------------------------------------------------------------

Migration is done. Let's continue with Tasks 4 and 5:

4. Update all current API calls to require new header to provide "X-API-Key".
5. Create new API calls: 
- POST: /consumer/data ("consumers" table - number of consumers will be low, initialy 1, later 2-3).
- POST: /consumer/api-key ("consumer_api_keys" - only one active api key per consumer).
- GET: /consumer/data
- GET: /consumer/api-key
- POST: /consumer/deactivate-api-key
- POST: /consumer/change-status

API response body structure skeleton should match current structure for other API calls, i.e. data + detail

- POST: /consumer/data
IN consumer_name
OUT consumer_id

- POST: /consumer/api-key
IN consumer_id
OUT api_key

- GET: /consumer/data
IN consumer_id
OUT name, status, created_at, updated_at

- GET: /consumer/api-key
IN api_key
OUT status, created_at, expires_at, updated_at

- POST: /consumer/deactivate-api-key
IN api_key
OUT 200 OK

- POST: /consumer/change-status
IN consumer_id, status
OUT 200 OK

Let's consider now only Tasks 3 and 4. Do not code, just make your suggestions and corrections (if needed) as an intergation architect.

----------------------------------------------------------------------------------------

Let's consider task 6:
6. Differentiate Rabbit queues by consumer_name, i.e. "customer_notification_[consumer_name]", i.e. "customer_notification_consumer001".

Do not code yet, just make your suggestions and corrections (if needed) as an intergation architect.

----------------------------------------------------------------------------------------

Some issues to fix:
1. I see that event types have different syntax, i.e. customer_creation and consumer.created - we need to similarize, I suggest using only "_"
2. I created customer with test consumer but in customer_events table consumer_id is shown that of admin - 00000000-0000-0000-0000-000000000001 - need to fix.
3. I created customer with test consumer but no new RabbitMQ queue was created with consumer_id - the message went to customer_notifications with routing key customer.customer.creation.system_default - need to fix.
4. Also I tried POST: events/redeliver as I saw some events with pending delivery status but got this error message "deliver_failure_reason": "TypeError: one of the hex, bytes, bytes_le, fields, or int arguments must be given" - need to check what's up with that.

----------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------